{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38fd21a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a54476",
   "metadata": {},
   "source": [
    "### **Step 1: We’ll write functions to load the binary data from the .idx files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cb08d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(num, 1, rows, cols)\n",
    "\n",
    "def read_labels(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        magic, num = struct.unpack(\">II\", f.read(8))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecb1814",
   "metadata": {},
   "source": [
    "### **Step 2: Create a Custom PyTorch Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0159d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, image_path, label_path):\n",
    "        self.images = torch.tensor(read_images(image_path), dtype=torch.float32) / 255.0\n",
    "        self.labels = torch.tensor(read_labels(label_path), dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a98ba2",
   "metadata": {},
   "source": [
    "### **Step 3: Load the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bd552fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"D:\\Code for tutorials\\Machine Learning\\datasets\\MNIST Dataset\"\n",
    "\n",
    "train_dataset = MNISTDataset(\n",
    "    image_path=os.path.join(base_path, \"train-images.idx3-ubyte\"),\n",
    "    label_path=os.path.join(base_path, \"train-labels.idx1-ubyte\")\n",
    ")\n",
    "\n",
    "test_dataset = MNISTDataset(\n",
    "    image_path=os.path.join(base_path, \"t10k-images.idx3-ubyte\"),\n",
    "    label_path=os.path.join(base_path, \"t10k-labels.idx1-ubyte\")\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19c1d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ceb009c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64385e3",
   "metadata": {},
   "source": [
    "Absolutely! Let’s **go through the data loading part line by line, word by word**, and explain what **each part means and does** in detail.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Part: `read_images` and `read_labels` Functions\n",
    "\n",
    "```python\n",
    "import struct\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "* `import struct`:\n",
    "  Imports Python's built-in `struct` module. This module helps **decode binary data** (like `.idx3-ubyte`) into Python values.\n",
    "\n",
    "* `import numpy as np`:\n",
    "  Imports the NumPy library with alias `np`. It's used for fast array and matrix operations.\n",
    "\n",
    "---\n",
    "\n",
    "### 📦 `read_images` Function\n",
    "\n",
    "```python\n",
    "def read_images(path):\n",
    "```\n",
    "\n",
    "* `def`: Starts a **function definition**.\n",
    "* `read_images`: The name of the function. This function **reads image data**.\n",
    "* `path`: A parameter that will hold the file path (like `train-images.idx3-ubyte`).\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    with open(path, 'rb') as f:\n",
    "```\n",
    "\n",
    "* `with`: Opens a context block for handling the file. It ensures the file is closed automatically.\n",
    "* `open(path, 'rb')`: Opens the file located at `path` in **binary read mode** (`rb` = read binary).\n",
    "* `as f`: Assigns the opened file to variable `f`.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "```\n",
    "\n",
    "* `struct.unpack(\">IIII\", ...)`:\n",
    "  This **unpacks** the first 16 bytes of the file into 4 integers:\n",
    "\n",
    "  * `>IIII`:\n",
    "\n",
    "    * `>` = big-endian byte order\n",
    "    * `I` = unsigned integer (4 bytes) — four times for 4 integers\n",
    "  * `f.read(16)`: Reads the first 16 bytes of the file\n",
    "* `magic`: A number identifying the file type (should be `2051` for images)\n",
    "* `num`: Number of images\n",
    "* `rows`: Number of rows per image (should be `28`)\n",
    "* `cols`: Number of columns per image (should be `28`)\n",
    "\n",
    "✅ At this point, we know how many images there are and their shape.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(num, 1, rows, cols)\n",
    "```\n",
    "\n",
    "* `f.read()`: Reads the rest of the file, which contains all image pixels\n",
    "* `np.frombuffer(...)`: Converts the binary data into a NumPy array\n",
    "* `dtype=np.uint8`: Each pixel is an unsigned 8-bit integer (0 to 255)\n",
    "* `.reshape(num, 1, rows, cols)`:\n",
    "\n",
    "  * `num`: number of images\n",
    "  * `1`: 1 channel (grayscale)\n",
    "  * `rows` & `cols`: 28x28 pixels\n",
    "  * Shape becomes `(N, 1, 28, 28)`, which is what PyTorch expects for image input.\n",
    "\n",
    "---\n",
    "\n",
    "### 📦 `read_labels` Function\n",
    "\n",
    "```python\n",
    "def read_labels(path):\n",
    "```\n",
    "\n",
    "Defines another function to load the **labels** (digits 0–9) from `.idx1-ubyte` files.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    with open(path, 'rb') as f:\n",
    "```\n",
    "\n",
    "Same as before: Opens the label file in binary mode.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        magic, num = struct.unpack(\">II\", f.read(8))\n",
    "```\n",
    "\n",
    "* Reads the first 8 bytes:\n",
    "\n",
    "  * `magic`: file identifier (should be `2049` for label files)\n",
    "  * `num`: number of labels\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8)\n",
    "```\n",
    "\n",
    "* Reads the rest of the file into a NumPy array of 8-bit unsigned integers (each representing a digit between 0–9)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧱 MNISTDataset Class\n",
    "\n",
    "```python\n",
    "class MNISTDataset(Dataset):\n",
    "```\n",
    "\n",
    "* Defines a **custom PyTorch dataset**.\n",
    "* `Dataset` is a base class from `torch.utils.data` that we’re extending.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    def __init__(self, image_path, label_path):\n",
    "```\n",
    "\n",
    "* `__init__`: The constructor runs when you create an instance of this class.\n",
    "* `image_path` & `label_path`: Parameters for the file paths.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        self.images = torch.tensor(read_images(image_path), dtype=torch.float32) / 255.0\n",
    "```\n",
    "\n",
    "* `read_images(image_path)`: Loads image data as a NumPy array.\n",
    "* `torch.tensor(..., dtype=torch.float32)`: Converts it to a PyTorch tensor of type `float32`\n",
    "* `/ 255.0`: Normalizes pixel values from `[0–255]` to `[0.0–1.0]`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        self.labels = torch.tensor(read_labels(label_path), dtype=torch.long)\n",
    "```\n",
    "\n",
    "* `read_labels(...)`: Loads label data\n",
    "* Converts to a PyTorch tensor of type `long` (required by `CrossEntropyLoss`)\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "```\n",
    "\n",
    "* Returns the number of samples (i.e., length of the dataset)\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "```\n",
    "\n",
    "* This method allows accessing a single `(image, label)` pair by index\n",
    "* Required for `DataLoader` to work\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Dataloader Code\n",
    "\n",
    "```python\n",
    "train_dataset = MNISTDataset(\n",
    "    image_path=os.path.join(base_path, \"train-images.idx3-ubyte\"),\n",
    "    label_path=os.path.join(base_path, \"train-labels.idx1-ubyte\")\n",
    ")\n",
    "```\n",
    "\n",
    "* Creates an object of your custom dataset class for **training data**\n",
    "* Loads both images and labels\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "```\n",
    "\n",
    "* Wraps your dataset in a `DataLoader`:\n",
    "\n",
    "  * `batch_size=64`: Each batch has 64 samples\n",
    "  * `shuffle=True`: Shuffles the dataset every epoch for better training\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want the **same word-by-word breakdown** for training, evaluation, or model definition too!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4e89a5",
   "metadata": {},
   "source": [
    "### **Step 5: Define a Simple Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1850d123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNN(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ef01b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[ 0.0207, -0.0087,  0.0202,  ...,  0.0051, -0.0087,  0.0154],\n",
       "                      [-0.0242, -0.0257,  0.0267,  ...,  0.0060, -0.0357, -0.0235],\n",
       "                      [-0.0100, -0.0350,  0.0344,  ..., -0.0110,  0.0331, -0.0077],\n",
       "                      ...,\n",
       "                      [ 0.0330, -0.0262,  0.0330,  ..., -0.0071,  0.0195,  0.0123],\n",
       "                      [ 0.0010,  0.0090, -0.0156,  ..., -0.0271, -0.0320,  0.0101],\n",
       "                      [-0.0173,  0.0200,  0.0264,  ...,  0.0134, -0.0336,  0.0217]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([ 2.1869e-02, -1.4737e-02,  3.0590e-02, -1.6426e-02,  1.8911e-02,\n",
       "                       5.7711e-03,  1.8876e-02, -6.3875e-03, -1.5170e-02, -3.2584e-02,\n",
       "                      -4.5517e-03, -2.5429e-02,  3.0657e-02,  3.2786e-02,  3.2774e-02,\n",
       "                      -1.2552e-02, -1.9630e-02,  2.1215e-02,  1.4565e-02, -1.3482e-02,\n",
       "                       7.1670e-03, -3.4134e-02, -1.8708e-03,  2.3108e-02, -5.6607e-03,\n",
       "                      -2.8381e-02,  2.5663e-02, -2.0219e-02,  3.0765e-02, -1.6686e-02,\n",
       "                       2.9755e-02,  2.1536e-02,  3.1963e-02,  2.5539e-02,  1.6697e-02,\n",
       "                      -2.3688e-02,  1.6919e-02, -3.0018e-02, -1.0890e-02,  2.1177e-03,\n",
       "                      -1.8944e-02, -3.2160e-02, -1.3051e-02,  3.6038e-03, -2.1521e-02,\n",
       "                      -1.4675e-02, -2.8970e-02, -3.4539e-02, -1.2939e-02,  1.6807e-02,\n",
       "                      -1.9230e-02,  3.1228e-02, -1.7656e-02, -1.7596e-02, -7.7109e-03,\n",
       "                      -1.1445e-02, -8.6807e-05, -3.5207e-02, -2.1807e-02, -8.1304e-03,\n",
       "                       5.3162e-03,  1.1528e-02, -2.0085e-02,  3.3635e-02,  3.1961e-02,\n",
       "                       2.2081e-02, -8.1184e-03, -2.8158e-02,  1.2998e-02,  3.1639e-02,\n",
       "                      -1.7196e-03, -3.7587e-03,  2.4066e-02, -2.9548e-02, -3.5204e-02,\n",
       "                       1.9516e-02,  7.6315e-03, -1.8208e-02, -2.0846e-02,  1.0664e-02,\n",
       "                      -7.6009e-03, -2.8790e-02,  2.8124e-02,  7.4481e-03, -9.9600e-03,\n",
       "                      -3.8392e-04,  1.1809e-02,  2.1877e-02,  1.5435e-03,  2.4524e-02,\n",
       "                       1.0989e-02,  2.3353e-02, -2.5924e-02,  3.2960e-02,  2.9888e-02,\n",
       "                      -3.2687e-02, -1.0448e-02, -2.6206e-02, -1.1293e-02, -1.0537e-02,\n",
       "                      -3.5607e-02, -1.7778e-02,  1.4211e-02,  1.3652e-02, -1.0213e-02,\n",
       "                      -3.7218e-03, -5.9592e-03, -1.6704e-02,  1.0431e-02,  3.3181e-02,\n",
       "                       3.5114e-02,  5.6787e-03, -1.5064e-02,  3.5621e-03, -8.5884e-03,\n",
       "                       2.2373e-02,  3.4292e-02, -3.9024e-04,  2.3680e-02,  2.5143e-02,\n",
       "                      -3.4875e-03, -5.4334e-03,  2.8942e-02,  1.8456e-02, -1.6141e-02,\n",
       "                      -2.5471e-02, -2.0608e-02,  2.7693e-02])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-0.0007,  0.0210,  0.0157,  ...,  0.0548, -0.0153,  0.0418],\n",
       "                      [ 0.0437,  0.0576, -0.0165,  ...,  0.0260,  0.0132, -0.0514],\n",
       "                      [ 0.0295,  0.0287, -0.0296,  ..., -0.0472,  0.0371, -0.0141],\n",
       "                      ...,\n",
       "                      [ 0.0398, -0.0096,  0.0216,  ..., -0.0503, -0.0103, -0.0370],\n",
       "                      [-0.0143, -0.0247, -0.0521,  ...,  0.0161,  0.0040, -0.0503],\n",
       "                      [-0.0708, -0.0166, -0.0588,  ..., -0.0378, -0.0473, -0.0071]])),\n",
       "             ('fc2.bias',\n",
       "              tensor([ 7.2800e-02, -2.9439e-02, -6.0787e-02, -8.5635e-02, -5.6021e-02,\n",
       "                      -2.2456e-02, -5.2577e-02, -3.8393e-02,  7.6292e-02,  4.6085e-02,\n",
       "                      -2.5694e-02,  7.9541e-02, -3.0347e-02,  6.1436e-02, -4.3732e-02,\n",
       "                       5.3072e-02,  1.8175e-03,  7.6713e-02,  5.1216e-02,  8.8210e-02,\n",
       "                      -5.5960e-02,  2.5801e-02,  3.4517e-02, -8.0752e-02,  4.3895e-02,\n",
       "                      -3.4723e-02,  6.8210e-02, -7.1404e-03,  3.0832e-02, -4.0832e-02,\n",
       "                      -3.9204e-02,  8.1200e-02,  2.8715e-02, -1.5431e-02, -6.6400e-02,\n",
       "                       2.8851e-02,  6.4833e-02,  2.0993e-02, -1.2172e-02,  7.0004e-02,\n",
       "                      -5.6931e-02, -7.6198e-02, -2.2396e-02,  5.6569e-02, -4.7797e-02,\n",
       "                       6.1899e-02,  7.6074e-02,  9.9342e-03,  4.7270e-03,  3.3621e-02,\n",
       "                      -6.5157e-02,  2.4440e-02, -1.8987e-03, -5.7891e-02,  7.1093e-05,\n",
       "                      -1.9982e-02,  7.7418e-02,  7.7909e-02,  1.9181e-02, -6.6486e-03,\n",
       "                      -8.5374e-02,  1.7078e-02,  2.2086e-02, -1.2368e-02])),\n",
       "             ('fc3.weight',\n",
       "              tensor([[-1.1511e-01, -2.2104e-02, -1.2555e-02, -1.3819e-02, -3.1395e-02,\n",
       "                        2.3530e-02, -1.1961e-01,  1.1093e-01,  7.5815e-03, -5.8928e-02,\n",
       "                        1.0123e-01,  1.0316e-01,  6.4203e-02,  9.7936e-02, -5.4617e-02,\n",
       "                       -1.1961e-03,  5.5997e-02, -9.7365e-02,  4.5726e-02, -6.0987e-02,\n",
       "                       -1.1755e-01,  4.5384e-02,  1.4330e-03, -5.6389e-02, -7.4607e-02,\n",
       "                       -7.7879e-02, -3.6356e-02,  1.4531e-02,  1.5582e-03, -1.0212e-01,\n",
       "                        3.7894e-02, -4.8530e-02,  2.3151e-02, -1.0659e-01, -3.9916e-02,\n",
       "                        6.8508e-02,  2.1174e-02, -4.0030e-02, -2.3063e-02,  5.9595e-02,\n",
       "                        2.3322e-02,  6.3820e-02,  1.0064e-01,  8.7866e-02,  9.4179e-02,\n",
       "                       -3.2420e-04, -6.1785e-02, -2.9888e-02, -1.9526e-02,  6.1701e-02,\n",
       "                        5.3887e-02,  6.6144e-02, -2.6775e-02, -2.0546e-03, -5.8298e-02,\n",
       "                       -3.2582e-02,  3.3749e-02, -6.7488e-02,  4.0844e-02,  1.1651e-02,\n",
       "                        1.1668e-01, -5.9356e-02, -3.7219e-02, -9.8080e-03],\n",
       "                      [ 1.4369e-02, -8.3114e-02,  5.4979e-02, -1.0483e-01,  1.0124e-01,\n",
       "                        1.5766e-02,  1.2811e-02, -8.1347e-02, -8.8774e-02,  5.0042e-02,\n",
       "                       -1.2461e-01,  7.5386e-02,  1.9038e-02, -4.7602e-03,  4.4158e-02,\n",
       "                       -1.0598e-01,  9.0110e-02,  4.4376e-03, -9.0372e-02,  4.8181e-02,\n",
       "                       -7.6048e-02, -8.4417e-02,  2.4212e-02,  6.1409e-02, -1.0484e-01,\n",
       "                        2.6348e-02,  6.2977e-02,  4.0461e-02, -1.1583e-01, -5.5841e-03,\n",
       "                        8.8213e-02,  1.2431e-01,  3.1225e-03,  5.6990e-02,  1.1620e-01,\n",
       "                        5.6020e-02,  9.3317e-02,  5.8341e-02, -1.0212e-01, -6.0944e-02,\n",
       "                        1.9956e-02,  7.8661e-02,  1.1597e-01,  2.0424e-02, -8.4960e-02,\n",
       "                        8.2902e-02, -1.4936e-02, -6.0783e-03,  7.3251e-02, -1.1874e-01,\n",
       "                        1.2454e-01, -1.0290e-01,  1.4045e-02, -9.9155e-02,  9.6528e-02,\n",
       "                       -9.3777e-02,  7.6639e-02,  5.3798e-02,  6.6226e-02, -6.0279e-02,\n",
       "                        1.1845e-01, -8.9083e-02,  8.6666e-02, -1.4307e-03],\n",
       "                      [ 7.6040e-02, -2.5402e-02,  4.0641e-02,  1.1030e-01, -2.5831e-02,\n",
       "                       -4.2737e-02, -1.2019e-01,  2.3074e-02,  1.1133e-01, -5.7229e-02,\n",
       "                        1.1176e-01, -8.2478e-02,  6.1277e-02, -8.8261e-02, -1.6450e-02,\n",
       "                        1.3055e-02,  2.0119e-02,  1.1232e-01, -5.0920e-02, -1.1204e-01,\n",
       "                        8.4015e-02, -5.3005e-03,  2.0068e-02, -5.9563e-02,  7.4233e-02,\n",
       "                       -3.0999e-02, -1.0871e-01,  7.7975e-02, -4.5471e-02,  8.3275e-02,\n",
       "                       -1.2218e-01, -3.2006e-02,  2.3513e-02, -7.2879e-02, -2.9084e-02,\n",
       "                        6.2157e-02,  1.6549e-02,  8.8154e-02,  1.7021e-02, -9.1967e-02,\n",
       "                       -9.9279e-02,  2.2779e-02,  8.2256e-02, -7.3681e-02,  6.1513e-02,\n",
       "                        2.0893e-02,  1.0257e-01, -9.0954e-03,  7.8768e-02, -7.0904e-02,\n",
       "                       -2.6856e-02, -6.0562e-02,  9.0295e-03,  8.2239e-02, -5.1591e-02,\n",
       "                       -1.1555e-01,  9.6203e-02,  7.0819e-03,  3.2749e-02, -7.2893e-02,\n",
       "                        3.8949e-02,  1.0291e-01, -2.3642e-02, -5.1127e-02],\n",
       "                      [ 9.8903e-02, -6.9807e-02,  4.0392e-02,  2.2147e-02, -6.0763e-02,\n",
       "                        9.5887e-03,  6.9573e-02,  2.9691e-02, -4.3878e-02,  2.5182e-02,\n",
       "                       -1.0728e-01, -8.6958e-02,  1.1071e-01, -6.9062e-02, -9.7714e-02,\n",
       "                       -9.1797e-02, -8.2171e-02, -1.1992e-01, -9.1746e-02, -8.3191e-02,\n",
       "                        1.1315e-01, -6.6667e-02,  1.0539e-01,  1.2362e-01,  3.1149e-03,\n",
       "                        2.2751e-02, -3.1683e-02, -5.1387e-02, -9.8191e-02, -8.6175e-02,\n",
       "                        5.5023e-02, -2.1143e-02,  6.8725e-02,  1.0314e-01, -1.0310e-01,\n",
       "                        9.9965e-02, -4.4472e-02, -2.5436e-02, -7.4762e-03, -2.3630e-02,\n",
       "                        2.7037e-02, -4.6840e-02,  6.5125e-02, -1.1399e-01,  5.6178e-03,\n",
       "                       -2.7728e-02,  2.7175e-02,  5.2304e-02, -8.4493e-02,  5.0923e-02,\n",
       "                       -4.2492e-02, -7.8098e-02, -5.4854e-02,  7.3434e-04,  1.0092e-03,\n",
       "                        3.8642e-02, -9.4053e-02,  1.9728e-02,  7.3039e-02, -6.3282e-02,\n",
       "                        1.4222e-02,  2.2998e-02,  1.7556e-02,  6.4158e-02],\n",
       "                      [ 7.8832e-02, -5.5264e-02,  8.5719e-02,  9.8186e-02, -9.6618e-02,\n",
       "                        6.8028e-03, -1.4290e-02, -6.4111e-04,  1.1333e-01,  8.5095e-02,\n",
       "                        9.8637e-02,  6.3058e-02, -2.4524e-02, -6.9493e-02, -5.3428e-02,\n",
       "                       -1.2379e-01,  8.1843e-02,  6.6034e-02, -7.9008e-03, -5.4051e-02,\n",
       "                        6.0320e-02, -8.3924e-02,  5.0982e-02, -8.5554e-02, -1.1765e-02,\n",
       "                       -3.2483e-02,  4.7175e-02, -9.3957e-02,  1.1651e-01,  8.7608e-02,\n",
       "                        1.0871e-01, -1.8451e-03,  1.0004e-01, -1.0655e-01, -7.9790e-02,\n",
       "                        1.0237e-01, -6.8120e-02,  1.0284e-01, -7.8215e-02,  5.0936e-02,\n",
       "                       -2.0610e-02, -1.0051e-01,  1.2740e-02,  5.1134e-02,  1.9309e-02,\n",
       "                       -1.0682e-01, -3.3684e-02, -9.0117e-02,  4.5366e-02, -7.8674e-03,\n",
       "                        1.1079e-01, -3.3465e-03, -3.0576e-02,  4.3905e-02, -1.0557e-01,\n",
       "                       -9.6254e-02,  4.7777e-02,  1.3842e-02,  1.1369e-01, -7.0122e-02,\n",
       "                        4.7225e-02, -8.9939e-02, -6.7222e-02, -3.4889e-02],\n",
       "                      [ 6.4861e-02,  1.1299e-01, -2.2073e-02, -5.6067e-02,  2.6204e-02,\n",
       "                        1.0873e-02,  4.3097e-02,  1.1025e-01, -3.8933e-02,  7.1945e-02,\n",
       "                       -3.1433e-02, -9.1834e-02,  4.6595e-02, -3.9308e-02,  1.0944e-01,\n",
       "                       -1.0551e-01, -5.0867e-02, -7.4180e-02, -5.3296e-02, -3.5711e-03,\n",
       "                        7.2870e-02, -1.1632e-01, -3.4441e-02, -1.7476e-02,  1.2317e-01,\n",
       "                        7.8068e-02, -8.8457e-02, -2.9604e-02,  2.0549e-02, -1.0073e-02,\n",
       "                       -3.2682e-02,  8.9980e-02,  1.6073e-02,  1.0289e-01, -9.8710e-02,\n",
       "                        8.0492e-02,  7.1629e-02, -7.2859e-02,  1.9138e-02,  6.7825e-02,\n",
       "                        7.7056e-03, -1.8341e-02,  3.5171e-02, -1.0580e-01, -7.9263e-02,\n",
       "                        5.3343e-03,  5.6735e-02, -3.2064e-02, -1.2451e-01, -7.1921e-02,\n",
       "                        8.1777e-02, -4.3191e-02,  6.4320e-02,  2.9081e-02,  2.7370e-02,\n",
       "                        3.9465e-02,  1.4492e-02, -1.1085e-01,  1.1490e-01,  1.1630e-01,\n",
       "                        1.1673e-01,  3.2031e-02, -6.5514e-02,  1.5304e-02],\n",
       "                      [ 1.0118e-01, -9.4796e-02,  1.1022e-01,  1.0532e-01,  1.2637e-02,\n",
       "                        6.7492e-02,  8.5369e-03, -5.6692e-02,  2.4364e-02,  4.2392e-03,\n",
       "                       -6.3430e-02, -2.6826e-03, -1.1144e-01, -9.4101e-02,  4.4053e-02,\n",
       "                        4.4666e-02, -7.5000e-02, -1.6316e-02, -8.9671e-02, -1.3853e-02,\n",
       "                        6.5887e-02,  2.8208e-03,  1.0241e-01,  7.2779e-03,  3.7896e-02,\n",
       "                       -4.9810e-02, -4.4296e-02, -7.2915e-02,  1.1925e-01,  2.2219e-03,\n",
       "                       -1.1000e-01,  9.3878e-02,  8.9966e-02, -3.3239e-03, -6.2274e-02,\n",
       "                       -4.1073e-02, -7.6902e-02,  1.1727e-01,  4.1239e-02, -8.1174e-02,\n",
       "                        6.1196e-02,  1.1107e-01, -8.2844e-02, -9.6497e-02, -2.7723e-02,\n",
       "                       -1.0817e-01, -6.4364e-03, -6.4190e-02, -6.5525e-02,  1.2225e-01,\n",
       "                        9.2811e-02,  2.5756e-02, -6.5125e-02, -1.1013e-01,  4.9063e-02,\n",
       "                        1.1573e-01, -2.1571e-02, -1.1997e-01, -1.1425e-01,  2.6461e-02,\n",
       "                        1.4583e-02,  9.1385e-02, -3.7164e-03,  5.2629e-03],\n",
       "                      [-4.1553e-02, -9.7189e-02, -1.1348e-01, -1.0980e-01, -2.8811e-03,\n",
       "                        7.8725e-02,  3.4361e-02, -9.2718e-02,  9.5642e-02, -3.7093e-02,\n",
       "                       -1.0993e-01, -9.7028e-03,  8.2607e-02,  2.8713e-02,  5.2119e-03,\n",
       "                       -4.5572e-02, -9.7507e-02,  2.9545e-02, -6.2678e-02,  6.2746e-02,\n",
       "                       -9.4245e-02,  9.3957e-02,  1.0438e-01, -9.6613e-02,  5.8052e-02,\n",
       "                       -1.3175e-02,  2.4470e-02, -5.3765e-02,  1.0752e-01, -1.0031e-01,\n",
       "                        2.3741e-02, -1.0343e-01,  3.5881e-02,  1.5753e-02,  1.9172e-02,\n",
       "                        4.6088e-02,  4.5920e-02, -4.6566e-02,  1.9405e-02, -7.0823e-02,\n",
       "                        5.8355e-02, -8.6880e-02, -7.8182e-02,  5.7095e-02,  6.5620e-02,\n",
       "                        1.2299e-02,  1.8430e-02,  1.2298e-01,  1.1922e-01,  2.7670e-03,\n",
       "                       -9.0202e-02, -1.1831e-01,  1.0948e-01, -2.8000e-02, -8.0142e-02,\n",
       "                       -5.1044e-02, -7.0939e-02,  9.0717e-02, -9.1384e-02,  1.6788e-02,\n",
       "                       -2.5377e-02, -4.8917e-03,  8.3905e-02, -8.2858e-03],\n",
       "                      [-3.3301e-02,  3.0527e-02,  3.7549e-02,  3.0401e-02,  7.2140e-02,\n",
       "                       -1.2320e-01, -8.9263e-02,  4.3068e-02, -2.5144e-02,  1.2117e-01,\n",
       "                        1.1914e-01, -3.1688e-02,  6.8229e-02,  4.4890e-02,  6.8805e-02,\n",
       "                        1.1179e-01,  5.7431e-02,  3.3624e-02,  6.3031e-02,  1.8239e-02,\n",
       "                        1.0068e-01, -2.9988e-03, -7.6498e-02,  2.3158e-02, -1.1270e-01,\n",
       "                        4.8619e-02, -8.4978e-02, -9.6376e-02, -7.5426e-02,  1.0574e-01,\n",
       "                        1.0690e-01,  3.5453e-02, -4.2725e-02, -1.1304e-01, -1.0601e-01,\n",
       "                        3.5999e-02, -1.1138e-01, -9.5856e-03, -9.3125e-02,  4.4141e-03,\n",
       "                       -1.1742e-01,  6.9304e-02, -7.4690e-02,  1.5586e-03,  5.5019e-02,\n",
       "                       -4.6166e-02,  9.9909e-02, -6.9444e-02,  5.1794e-02, -9.5045e-02,\n",
       "                        1.1989e-01, -1.1502e-01, -9.0520e-02,  6.8550e-02, -1.1565e-01,\n",
       "                        3.1319e-02, -8.3008e-02,  1.8805e-02, -6.1314e-02, -7.3302e-02,\n",
       "                       -1.0336e-01,  5.0299e-02,  9.3964e-02, -1.1778e-01],\n",
       "                      [-7.2309e-02,  1.0673e-01, -9.0594e-02,  1.0650e-02, -1.0248e-01,\n",
       "                       -8.0732e-02, -4.9064e-02, -5.9240e-02, -1.1078e-02, -1.2077e-01,\n",
       "                        1.1928e-01,  6.7807e-02,  6.9568e-02,  9.8322e-02,  4.5737e-03,\n",
       "                        6.7397e-02,  8.3073e-02,  8.9940e-02,  3.3634e-02, -5.3065e-02,\n",
       "                        1.1110e-01,  8.8597e-02, -9.7158e-02, -5.8632e-02,  2.3027e-02,\n",
       "                       -4.3524e-02,  1.0122e-02,  9.7184e-02, -1.1004e-01, -6.7567e-03,\n",
       "                        9.0709e-02,  4.6927e-02,  4.5712e-02,  7.3892e-02, -3.3938e-02,\n",
       "                       -2.1431e-02, -7.8838e-02,  9.0674e-02,  1.0472e-01, -6.9087e-02,\n",
       "                       -1.0530e-01,  5.7538e-02,  1.9578e-02,  2.5248e-02,  1.4274e-02,\n",
       "                        2.4611e-02, -9.3593e-02,  3.3400e-02,  2.2432e-02,  2.3197e-02,\n",
       "                        8.2413e-02,  9.0459e-02,  6.2223e-02,  1.0194e-01,  5.9914e-02,\n",
       "                        8.2647e-02,  1.0529e-01, -7.1362e-05,  3.7741e-02, -4.5152e-02,\n",
       "                       -1.2257e-01,  1.1155e-01, -1.1959e-01,  2.2384e-02]])),\n",
       "             ('fc3.bias',\n",
       "              tensor([ 0.0019, -0.0831,  0.0202, -0.0403, -0.0201,  0.0741, -0.0841,  0.0094,\n",
       "                       0.0718, -0.0489]))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12c29dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of SimpleNN(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "197acad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd235bc4",
   "metadata": {},
   "source": [
    "### **Step 6: Loss and Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c87f4666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(),\n",
    "                            lr=0.01 #learning rate = possibly the most important hyperparameter\n",
    "                            )\n",
    "print(optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978cd1f4",
   "metadata": {},
   "source": [
    "Perfect! Let’s now dive into **Step 5 (Model definition)** and **Step 6 (Loss & Optimizer setup)** with a **word-by-word, line-by-line explanation** just like before.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Step 5: Define a Simple Neural Network\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "```\n",
    "\n",
    "* `import torch.nn as nn`:\n",
    "  Imports PyTorch’s **neural network module** as `nn`.\n",
    "  This includes building blocks like `Linear`, `Conv2d`, `ReLU`, etc.\n",
    "\n",
    "* `import torch.nn.functional as F`:\n",
    "  Imports PyTorch’s **functional API** under alias `F`.\n",
    "  This gives access to activation functions like `F.relu()`, `F.softmax()`, etc. — used in forward passes.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧱 Define the Model Class\n",
    "\n",
    "```python\n",
    "class SimpleNN(nn.Module):\n",
    "```\n",
    "\n",
    "* `class SimpleNN`:\n",
    "  Defines a **new neural network class** called `SimpleNN`.\n",
    "\n",
    "* `(nn.Module)`:\n",
    "  Inherits from PyTorch’s base class for models: `nn.Module`.\n",
    "  This gives our model structure, parameter tracking, etc.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "```\n",
    "\n",
    "* `def __init__(self):`\n",
    "  Constructor method: This is run when we create a new model object.\n",
    "\n",
    "* `super(SimpleNN, self).__init__()`:\n",
    "  Calls the constructor of the **parent class** (`nn.Module`) to set up model infrastructure like `parameters`, `state_dict`, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 Define Layers\n",
    "\n",
    "```python\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "```\n",
    "\n",
    "* `self.fc1`: First fully connected layer.\n",
    "* `nn.Linear(28*28, 128)`:\n",
    "\n",
    "  * Input size = 784 (i.e., 28 × 28 flattened image)\n",
    "  * Output size = 128 neurons\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "```\n",
    "\n",
    "* Second fully connected layer\n",
    "* Takes input of 128 (from `fc1`), outputs 64 neurons\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "```\n",
    "\n",
    "* Third and final layer\n",
    "* Outputs 10 values (one for each digit class 0–9)\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Forward Propagation Function\n",
    "\n",
    "```python\n",
    "    def forward(self, x):\n",
    "```\n",
    "\n",
    "* `def forward(self, x)`:\n",
    "  Defines how input `x` flows through the layers — i.e., **the forward pass**.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        x = x.view(-1, 28*28)\n",
    "```\n",
    "\n",
    "* `x.view(-1, 28*28)`:\n",
    "\n",
    "  * Reshapes the 2D image (`[B, 1, 28, 28]`) into a flat 1D vector (`[B, 784]`)\n",
    "  * `-1` means \"infer batch size\"\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        x = F.relu(self.fc1(x))\n",
    "```\n",
    "\n",
    "* Passes `x` through `fc1`, then applies **ReLU activation**\n",
    "* `F.relu(...)`: Applies Rectified Linear Unit: `max(0, x)`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        x = F.relu(self.fc2(x))\n",
    "```\n",
    "\n",
    "* Same: pass through second fully connected layer and apply ReLU\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        x = self.fc3(x)\n",
    "```\n",
    "\n",
    "* Pass through the third layer (no activation here because it will go to CrossEntropyLoss)\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        return x\n",
    "```\n",
    "\n",
    "* Return the **raw output logits** — not probabilities.\n",
    "  These will be converted to probabilities internally by `CrossEntropyLoss`.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Create Model Instance\n",
    "\n",
    "```python\n",
    "model = SimpleNN()\n",
    "```\n",
    "\n",
    "* Instantiates the `SimpleNN` class\n",
    "* `model` now holds the neural network and its parameters\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Step 6: Define Loss and Optimizer\n",
    "\n",
    "```python\n",
    "import torch.optim as optim\n",
    "```\n",
    "\n",
    "* Imports the **optimizer module** in PyTorch.\n",
    "* This provides optimizers like SGD, Adam, RMSProp, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### 📉 Define Loss Function\n",
    "\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "```\n",
    "\n",
    "* `criterion`: A variable holding the **loss function**.\n",
    "* `nn.CrossEntropyLoss()`:\n",
    "\n",
    "  * Combines **softmax + negative log likelihood**\n",
    "  * Used for **multi-class classification**\n",
    "  * Expects **raw logits as input** and **class index as target**\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 Define Optimizer\n",
    "\n",
    "```python\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "```\n",
    "\n",
    "* `optimizer`: A variable holding the **optimizer object**.\n",
    "* `optim.SGD(...)`: Stochastic Gradient Descent\n",
    "* `model.parameters()`: Gives the optimizer all the model’s weights to update during training\n",
    "* `lr=0.01`: Learning rate — controls **how fast** the model learns (step size in weight updates)\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Component           | Meaning                     |\n",
    "| ------------------- | --------------------------- |\n",
    "| `fc1`, `fc2`, `fc3` | Fully connected layers      |\n",
    "| `F.relu`            | Activation function         |\n",
    "| `CrossEntropyLoss`  | Loss for classification     |\n",
    "| `SGD`               | Optimizer to adjust weights |\n",
    "| `x.view(...)`       | Flatten the image           |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to break down **Step 7 (training loop)** next in the same style? That part is the heart of model learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decc427b",
   "metadata": {},
   "source": [
    "### **Step 7: Train the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a7947",
   "metadata": {},
   "source": [
    "### Building a traing loop and a testing loop in PyTorch\n",
    "\n",
    "A couple of things we need in a training loop:\n",
    "0. Loop through data\n",
    "1. forward pass (this involves data moving through our model's forward() functions) to make predictions on data - also called forward propagation\n",
    "2. calculate the loss (compare forward pass predictions to ground truth labels)\n",
    "3. optimizer zero grad\n",
    "4. loss.backward() - move backwards through the network to calculate the gradients of each of the parameters of our model with respect to the loss (**backpropagation**)\n",
    "5. optimizer.step() - use the optimizer to adjust our model's parameters to try and improve the loss (**gradient descent**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b45d394f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.7709\n",
      "Epoch [2/5], Loss: 0.5747\n",
      "Epoch [3/5], Loss: 0.3954\n",
      "Epoch [4/5], Loss: 0.3435\n",
      "Epoch [5/5], Loss: 0.3153\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a36b566",
   "metadata": {},
   "source": [
    "### **Step 8: Evaluate Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94043201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.46%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f86401c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebd11d2a",
   "metadata": {},
   "source": [
    "Awesome! Let’s continue with the **step-by-step, word-by-word breakdown** of:\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ Step 7: **Training the Model**\n",
    "\n",
    "# ✅ Step 8: **Evaluating the Model**\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Step 7: Training the Model\n",
    "\n",
    "Here's the full training loop code first:\n",
    "\n",
    "```python\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔢 `epochs = 5`\n",
    "\n",
    "* `epochs`: Number of **complete passes** through the training dataset.\n",
    "* `= 5`: We’ll train the model for 5 full rounds.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 `for epoch in range(epochs):`\n",
    "\n",
    "* `for ... in range(epochs)`: A loop that will repeat `epochs` times (i.e., 5).\n",
    "* `epoch`: This variable keeps track of which round (0 to 4) we’re in.\n",
    "\n",
    "---\n",
    "\n",
    "### `running_loss = 0.0`\n",
    "\n",
    "* This variable stores the **total loss** for the current epoch.\n",
    "* We’ll use this to calculate and print **average loss** at the end of the epoch.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔂 `for images, labels in train_loader:`\n",
    "\n",
    "* `train_loader`: The DataLoader that returns batches of data.\n",
    "* `images`: A batch of input images (e.g., shape `[64, 1, 28, 28]`)\n",
    "* `labels`: Corresponding ground-truth labels (e.g., `[3, 5, 1, 7, ...]`)\n",
    "* This inner loop goes over **one batch at a time**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧼 `optimizer.zero_grad()`\n",
    "\n",
    "* Clears the **previous gradients** stored in the optimizer.\n",
    "* This is critical: PyTorch accumulates gradients by default, so we need to reset them each step.\n",
    "\n",
    "---\n",
    "\n",
    "### 🤖 `outputs = model(images)`\n",
    "\n",
    "* Pass the batch of `images` through the model (calls `forward()` method)\n",
    "* `outputs`: Model’s raw predictions (called **logits**), shape `[batch_size, 10]`\n",
    "\n",
    "---\n",
    "\n",
    "### 📉 `loss = criterion(outputs, labels)`\n",
    "\n",
    "* Calculates the **loss** between the predicted `outputs` and the `labels`.\n",
    "* `criterion` is `nn.CrossEntropyLoss()` which internally applies **softmax** + **negative log likelihood**.\n",
    "* `loss`: A single scalar value showing **how wrong** the predictions were.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 `loss.backward()`\n",
    "\n",
    "* Computes the **gradients** of loss with respect to model weights.\n",
    "* This is the **backpropagation** step.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 `optimizer.step()`\n",
    "\n",
    "* Updates the model parameters using the gradients computed.\n",
    "* This is where the model **learns**.\n",
    "\n",
    "---\n",
    "\n",
    "### ➕ `running_loss += loss.item()`\n",
    "\n",
    "* `loss.item()`: Converts PyTorch tensor to a Python float.\n",
    "* Accumulates the total loss for the current epoch.\n",
    "\n",
    "---\n",
    "\n",
    "### 🖨️ `print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")`\n",
    "\n",
    "* Prints average loss for the epoch.\n",
    "* `epoch+1`: Epochs start from 0 in Python, so we add 1 to show human-readable index.\n",
    "* `running_loss / len(train_loader)`: Computes average loss across all batches.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Step 8: Evaluating the Model\n",
    "\n",
    "Here’s the code:\n",
    "\n",
    "```python\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔢 `correct = 0` and `total = 0`\n",
    "\n",
    "* `correct`: To count how many predictions were correct.\n",
    "* `total`: To count total number of test images.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ `with torch.no_grad():`\n",
    "\n",
    "* Disables gradient tracking.\n",
    "* Saves memory and speeds up evaluation (no learning happens here).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 `for images, labels in test_loader:`\n",
    "\n",
    "* Loops through batches from the **test set** (not training set).\n",
    "\n",
    "---\n",
    "\n",
    "### 🤖 `outputs = model(images)`\n",
    "\n",
    "* Pass images through the model to get predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 `_, predicted = torch.max(outputs.data, 1)`\n",
    "\n",
    "* `outputs.data`: Gets raw prediction scores.\n",
    "* `torch.max(..., 1)`: Finds the **index of the maximum value** in each row (i.e., the predicted class).\n",
    "* `_`: We ignore the actual max value.\n",
    "* `predicted`: The model’s predicted class labels.\n",
    "\n",
    "---\n",
    "\n",
    "### ➕ `total += labels.size(0)`\n",
    "\n",
    "* `labels.size(0)`: Number of samples in the batch.\n",
    "* We add that to `total`.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ `correct += (predicted == labels).sum().item()`\n",
    "\n",
    "* `(predicted == labels)`: Boolean tensor where `True` for correct predictions.\n",
    "* `.sum()`: Total number of correct predictions in this batch.\n",
    "* `.item()`: Converts it to a Python number.\n",
    "* Accumulates correct predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### 🖨️ `print(f\"Accuracy: {100 * correct / total:.2f}%\")`\n",
    "\n",
    "* Calculates accuracy = `(correct / total) * 100`\n",
    "* Formats to 2 decimal places using `:.2f`\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "| Concept             | Meaning                           |\n",
    "| ------------------- | --------------------------------- |\n",
    "| `zero_grad()`       | Reset gradients before backprop   |\n",
    "| `loss.backward()`   | Compute gradients                 |\n",
    "| `optimizer.step()`  | Apply gradients to weights        |\n",
    "| `torch.no_grad()`   | Turns off autograd for inference  |\n",
    "| `torch.max(..., 1)` | Get predicted class               |\n",
    "| `accuracy`          | Percentage of correct predictions |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to:\n",
    "\n",
    "* **Plot predictions**, or\n",
    "* **Save/load the model**, or\n",
    "* **Visualize losses/accuracy**, or\n",
    "* Try a **CNN** model next?\n",
    "\n",
    "Let me know what you want to build next from here 👇\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
